def define_model_conv_bigru(model_params: dict):
    input_block = tf.keras.layers.Input(shape=(model_params['window_size'],
        model_params['output_sequence_length']), dtype=tf.int32, name=
        'input_block')
    x = tf.keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()
        ), output_dim=model_params['embedding_dimension'], input_length=
        model_params['output_sequence_length'], mask_zero=False, name='embed')(
        input_block)
    x = tf.keras.layers.BatchNormalization(name='embed_batch_norm')(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=
        model_params['conv1d_0_units'], kernel_size=model_params[
        'conv1d_0_kernelsize'], padding=model_params['conv1d_0_padding'],
        activation=model_params['conv1d_0_activation'], name='conv1d_0'))(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=
        model_params['conv1d_1_units'], kernel_size=model_params[
        'conv1d_1_kernelsize'], padding=model_params['conv1d_1_padding'],
        activation=model_params['conv1d_1_activation'], name='conv1d_1'))(x)
    x = tf.keras.layers.Dropout(rate=model_params['drop_0_rate'])(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(model_params[
        'gru_0_units'], return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(model_params[
        'gru_1_units'], return_sequences=False))(x)
    x = tf.keras.layers.Dense(units=3, activation='softmax')(x)
    model = tf.keras.models.Model(inputs=input_block, outputs=x)
    model.compile(optimizer=tf.keras.optimizers.Adam(model_params[
        'initial_lr']), loss=tf.keras.losses.SparseCategoricalCrossentropy(
        ), metrics=['accuracy'])
    return model
